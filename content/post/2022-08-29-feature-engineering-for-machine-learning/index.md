---
title: Feature Engineering for Machine Learning
author: Shuqiang
date: '2022-08-29'
slug: []
categories:
  - Python
tags:
  - Machine Learning
subtitle: '这本书是由爱丽丝·郑、阿曼达·卡萨丽著，陈光欣译的一篇特征工程类书籍，主要通过Python示例掌握特征工程基本原则和实际以你够用，增强机器学习算法效果；该书在介绍特征工程的同时，也介绍了NLP中的词袋/tf-idf及CV中的图像处理等内容'
description: ''
image: ''
---

> 在本次学习中，除了掌握相关概念外，我们还需要知道为什么会/要这么做

# 第1章：机器学习流程

# 第2章：数值转换

# 第3章：文本数据：扁平化、过滤和分块

# 第4章：特征缩放的效果：从词袋到tf-idf

tf-idf(w,d)=bow(w,d)\*log(N/单词w出现在其中文档的数量)

在Python的sklearn中，tf-idf的计算做了更多的[平滑处理手段](https://zhuanlan.zhihu.com/p/480672270)

1.  无论是tf-idf还是L2正则化，都是特征缩放的一部分，其真正的用武之地是加快收敛速度
2.  tf-idf可以比较彻底地消除没有信息量的单次

# 第5章：分类变量：自动化时代的数据计数

<table><tr><td bgcolor=#CCEB85> <font color=blue>分类变量</font>用来表示类别/标记的变量类型，与定量变量相比，分类变量可以告诉我们观测对象在某一维度上是否相同，而定量变量则告诉我们观测对象在某一维度上有多大的不同。本章的主要内容：其一，常用的3种分类变量的编码类型，分别是<strong>ont-hot编码、虚拟编码、效果编码</strong>，并从可解释性/缺失数据处理/计算和存储成本的角度进行比较，想Pandas和scikit-learn更偏好ont-hot和dummy编码；其二，以上三种编码方式都不适用于处理大型分类变量(什么问题？)，对于大型分类变量(常见于定向广告/欺诈检测应用)，一种方式是选择易训练、简单的模型，另一种则是通过压缩特征(特征散列化和分箱计数)方式



### 5.1 分类编码

##### 5.1.1 one-hot编码

使用一组比特位(即由0,1构成的向量)表示，并且每一个观测只能属于唯一一类。因此一个可能有k个类别的分类变量可编码成长度为k(e_1,e~2~,e~3~...e~k~)的特征向量，因此one-hot使用的比特位比实际需要的多一位，公式如下，但公式表现出明显的线性相关性：
$$
e_1+e_2+e_3+...+e_k=1
$$
Python实现：sklearn.processing.OneHotEncoder

优缺点：使得训练出的模型不唯一，但是特征的不同线性组合可以得到同样的预测

##### 5.1.1 虚拟编码(dummy)

one-hot允许有$$k$$个自由度，而变量本身只需要$$k-1$$个自由度。没有被使用的特征通过一个全零向量表示，称为**参照类**，特征向量同样是由0和1构成。

Python实现：pandas.get_dummies

优缺点：有参照类就有更强的解释性，在线性模型中，使用one-hot编码得到的截距项表示$$y$$的整体均值，而dummy编码时得到的截距项表示$$y$$对于参照类的均值。但二者中不同类别下的y均值差距一样，尽管线性模型的斜率系数和截距项均有所不同

##### 5.1.3 效果编码

同虚拟编码相似，区别在于其参照类全部由$$-1$$组成的向量表示，这样其线性回归模型更容易解释，各系数表示了各个类别的均值与整体均值之间的差(这称为该类别/水平的主效果，效果编码由此而来)



|                | one-hot编码                                                  | 虚拟编码                                               | 效果编码                                         |
| -------------- | ------------------------------------------------------------ | ------------------------------------------------------ | ------------------------------------------------ |
| 编程冗余       | 有                                                           | 无                                                     | 无                                               |
| 可解释性       | 因为得到的训练模型非唯一性，有时候难以解释                   | 可生成唯一可解释的模型，但相对定量表达值有时候不够直观 | 可生成唯一可解释的模型                           |
| 缺失值处理     | 处理好，因为每一种类型都有唯一的特征向量，而缺失值可以用零特征向量来表示，模型输出也是总体均值 | 不好，因为在限定条件下，                               |                                                  |
| 计算和存储成本 | 无                                                           | 无                                                     | 全是-1组成的向量是个密集矩阵，导致较高的计算成本 |



### 5.2 处理大型分类变量

压缩特征

1. 特征散裂化

2. 分箱计数

### 5.3 小结

# 第6章：数据降维：使用PCA挤压数据

# 第7章：非线性特征化与K-均值模型堆叠

# 第8章：自动特征生成：图像特征提取与深度学习

# 第9章：回到特征：建立学术论文推荐器

# 附录A：线性建模与线性代数基础
